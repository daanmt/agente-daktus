# üî¥ RELAT√ìRIO CONSOLIDADO: Sistema de Mem√≥ria & Feedback

**Data**: 2025-12-11  
**Para**: Claude Code (continua√ß√£o da implementa√ß√£o)  
**Contexto**: Sistema de aprendizado n√£o est√° funcionando efetivamente  
**Prioridade**: CR√çTICA  

---

## üìä SUM√ÅRIO EXECUTIVO

### Problema Central
O agente **n√£o aprende efetivamente** com o feedback do usu√°rio, resultando em:
- ‚ùå Sugest√µes rejeitadas continuam reaparecendo
- ‚ùå Desperd√≠cio de 40-60% dos tokens em sugest√µes irrelevantes
- ‚ùå Usu√°rio revisa os mesmos erros repetidamente
- ‚ùå ROI do sistema degradando ao longo do tempo

### Estado Atual (Dev History Review)
**Waves Completas** (segundo roadmap.md):
- ‚úÖ Wave 1: Clinical Safety Foundations (Pydantic, AST validation, LLM contracts)
- ‚úÖ Wave 2: Memory & Learning (declarado completo, mas COM BUGS)
- ‚úÖ Wave 3: Observability & Cost Control (cost tracking, audit reports)

**Discrep√¢ncia Identificada**:
O roadmap declara Wave 2 completa, mas `proud-seeking-newt.md` documenta **3 bugs cr√≠ticos** que quebram o learning loop:

1. **Reconstruction Display Bug** - Mudan√ßas mostram "N/A" em vez de valores reais
2. **Learning System Not Working** - Padr√µes de rejei√ß√£o n√£o s√£o aplicados
3. **Feedback UX Too Complex** - 7 op√ß√µes confundem usu√°rio (deve ser 3)

---

## üîç AN√ÅLISE DETALHADA DOS PROBLEMAS

### Problema 1: Reconstruction Display Bug

**Sintoma**:
```
MUDAN√áAS APLICADAS
~ MODIFIED: N/A
  N/A
```

**Causa Raiz** (proud-seeking-newt.md):
```python
# Producer (protocol_reconstructor.py:366-395)
changes.append({
    "suggestion_id": ...,  # ‚ùå Chave errada
    "title": ...,          # ‚ùå Chave errada
})

# Consumer (display_manager.py:254-310)
change_type = change.get("type", "modified")  # ‚úÖ Chave esperada
location = change.get("location", "N/A")      # ‚úÖ Chave esperada
```

**Status**: üî¥ N√ÉO CORRIGIDO (n√£o encontrado em dev_history.md)

**Impacto**: Usu√°rio n√£o v√™ quais mudan√ßas foram aplicadas ‚Üí zero confian√ßa

**Localiza√ß√£o para Claude Code**:
- `src/agent/applicator/protocol_reconstructor.py` (linhas 366-395)
- `src/agent/cli/display_manager.py` (linhas 254-310)

---

### Problema 2: Learning System Not Working (CR√çTICO)

**Sintoma**: Sugest√µes rejeitadas continuam reaparecendo

**4 Causas Ra√≠zes Identificadas**:

#### 2.1 Threshold Muito Alto
```python
# enhanced.py:335
active_filters = self.memory_qa.get_active_filters(min_frequency=3)
# Pattern com frequency=1 √© IGNORADO porque 1 < 3
```

**Status**: üü° PARCIALMENTE CORRIGIDO
- Roadmap menciona "Threshold=1 para ativa√ß√£o imediata" (linha 45)
- Mas proud-seeking-newt.md indica que ainda est√° em 3
- **Verifica√ß√£o necess√°ria**: Claude Code deve confirmar o valor atual

#### 2.2 Filtros N√£o Sempre no Prompt
```python
# enhanced.py:335-340
filter_instructions = self._build_filter_instructions(active_filters)
# ‚ùå Constru√≠do mas NEM SEMPRE inclu√≠do no prompt
```

**Status**: üî¥ N√ÉO CORRIGIDO

#### 2.3 Post-Filtering Apenas por Keywords
```python
# enhanced.py:581-684
if keyword.lower() in text_to_check:  # ‚ùå Busca literal apenas
    should_keep = False

# Blocklist: ["desnecess√°rio", "redundante", "irrelevante"]
# Padr√£o "m√©dico deve ter op√ß√£o de prescrever" ‚Üí N√ÉO DETECTADO
```

**Status**: üî¥ N√ÉO CORRIGIDO

#### 2.4 Relat√≥rios EDITED N√£o S√£o Usados
```
Fluxo Atual (QUEBRADO):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ An√°lise 1 ‚Üí Feedback ‚Üí EDITED report gerado   ‚îÇ
‚îÇ                            ‚Üì                    ‚îÇ
‚îÇ An√°lise 2 usa ORIGINAL (‚ùå) n√£o EDITED (‚úÖ)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Status**: üî¥ N√ÉO CORRIGIDO

**Impacto Combinado**: Sistema n√£o aprende, desperdi√ßa tokens, frustra usu√°rio

---

### Problema 3: Feedback UX Too Complex

**Atual**: 7 op√ß√µes (S/N/E/C/P/Q)  
**Necess√°rio**: 3 op√ß√µes (Relevante/Irrelevante/Sair)

**Status**: üî¥ N√ÉO CORRIGIDO

**Localiza√ß√£o para Claude Code**:
- `src/agent/feedback/feedback_collector.py` (linhas 150-250)

---

## üèóÔ∏è ARQUITETURA: Estado Atual vs Desejado

### Estado Atual (Arquivos que DEVEM Existir segundo Roadmap)

**Wave 2 - Memory & Learning (declarado completo)**:
```
src/agent/learning/
‚îú‚îÄ‚îÄ rules_engine.py         # ‚ùì Exist√™ncia a verificar
‚îú‚îÄ‚îÄ feedback_learner.py     # ‚ùì Exist√™ncia a verificar
‚îî‚îÄ‚îÄ models.py               # ‚ùì N√£o mencionado

src/agent/validators/
‚îî‚îÄ‚îÄ reference_validator.py  # ‚ùì Exist√™ncia a verificar

src/agent/applicator/
‚îî‚îÄ‚îÄ change_verifier.py      # ‚ùì Exist√™ncia a verificar
```

**A√ß√µes para Claude Code**:
1. Verificar se estes arquivos existem
2. Se existem, verificar se t√™m bugs de implementa√ß√£o
3. Se n√£o existem, criar conforme especifica√ß√£o deste relat√≥rio

---

### Estado Desejado (Arquitetura Completa)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AGENTE DAKTUS QA                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              MEMORY & LEARNING SYSTEM                       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Layer 1: HARD RULES (Blocking)                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Reference whitelist (playbook only)                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Structural constraints (JSON schema)               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Forbidden patterns (autonomy invasion)             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Üí BLOQUEIA antes de gerar sugest√£o                   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                         ‚Üì                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Layer 2: SOFT RULES (Filtering)                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Pattern-based filters (semantic matching)          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Confidence scoring                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Üí FILTRA sugest√µes geradas                           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                         ‚Üì                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Layer 3: LEARNING CORPUS (Context)                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Historical feedback (memory_qa.md)                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Best practices                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Üí ENRIQUECE contexto do LLM                          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã PLANO DE A√á√ÉO PARA CLAUDE CODE

### FASE 0: Verifica√ß√£o e Emergency Fixes (2-4 horas)

#### Tarefa 0.1: Auditoria de Arquivos
```bash
# Verificar exist√™ncia dos arquivos Wave 2
ls -la src/agent/learning/
ls -la src/agent/validators/
ls -la src/agent/applicator/change_verifier.py

# Se n√£o existirem, criar estrutura base
mkdir -p src/agent/learning
mkdir -p src/agent/validators
```

#### Tarefa 0.2: Fix #1 - Reconstruction Display
**Arquivo**: `src/agent/applicator/protocol_reconstructor.py`  
**Linhas**: 366-395

**Mudan√ßa**:
```python
# ANTES:
changes.append({
    "suggestion_id": sug.get("id", "N/A"),
    "title": sug.get("title", "N/A"),
    "category": sug.get("category", "N/A"),
    "status": "applied"
})

# DEPOIS:
changes.append({
    "type": "modified",  # ou determinar do tipo de mudan√ßa
    "location": f"Node: {node_id} | Question: {question_uid}",
    "description": sug.get('description', sug.get('title', 'N/A'))[:200]
})
```

**Teste**:
```bash
# Executar reconstru√ß√£o
python run_agent.py

# Verificar que "MUDAN√áAS APLICADAS" mostra valores reais
# N√ÉO deve mostrar "N/A"
```

---

#### Tarefa 0.3: Fix #2.1 - Lower Threshold
**Arquivo**: `src/agent/analysis/enhanced.py`  
**Linha**: 335

**Mudan√ßa**:
```python
# ANTES:
active_filters = self.memory_qa.get_active_filters(min_frequency=3)

# DEPOIS:
min_freq = int(os.getenv("MEMORY_MIN_FREQUENCY", "1"))
active_filters = self.memory_qa.get_active_filters(min_frequency=min_freq)
```

**Verifica√ß√£o**:
```python
# Ap√≥s feedback negativo, verificar memory_qa.md
# Pattern deve ter frequency=1
# E na pr√≥xima an√°lise, active_filters deve inclu√≠-lo
```

---

#### Tarefa 0.4: Fix #2.2 - Garantir Filtros no Prompt
**Arquivo**: `src/agent/analysis/enhanced.py`  
**Linhas**: 368-371

**Mudan√ßa**:
```python
# ANTES (condicional):
if active_filters:
    prompt += filter_instructions

# DEPOIS (sempre):
prompt += f"""
---
FILTROS ATIVOS (Baseados em Feedback do Usu√°rio):
{filter_instructions if active_filters else "Nenhum filtro ativo ainda."}
---
"""
```

---

#### Tarefa 0.5: Fix #2.3 - Pattern-Based Filtering
**Arquivo**: `src/agent/analysis/enhanced.py`  
**Linhas**: 581-684 (m√©todo `_apply_post_filters`)

**Adicionar novo m√©todo**:
```python
def _matches_rejection_pattern(self, suggestion: dict) -> tuple[bool, str]:
    """Detecta padr√µes sem√¢nticos de rejei√ß√£o."""
    text = f"{suggestion.get('title', '')} {suggestion.get('description', '')}".lower()
    
    patterns = {
        "autonomy_invasion": [
            "priorizar", "deve usar", "preferir", "em vez de",
            "substituir por", "trocar por", "obrigatoriamente"
        ],
        "out_of_scope": [
            "n√£o est√° no playbook", "adicionar medicamento",
            "introduzir novo", "implementar funcionalidade",
            "tooltip", "interface", "nova tela"
        ],
        "already_implemented": [
            "j√° existe", "j√° implementado", "j√° temos",
            "j√° est√° presente"
        ]
    }
    
    for pattern_name, keywords in patterns.items():
        if any(kw in text for kw in keywords):
            return True, pattern_name
    
    return False, ""
```

**Integrar no `_apply_post_filters`**:
```python
# Ap√≥s keyword filtering, adicionar:
is_pattern_match, pattern_name = self._matches_rejection_pattern(sug)
if is_pattern_match:
    filtered.append({
        **sug,
        "filter_reason": f"Padr√£o de rejei√ß√£o: {pattern_name}"
    })
    logger.warning(f"‚ö†Ô∏è Sugest√£o bloqueada por padr√£o: {sug['id']} ({pattern_name})")
    continue
```

---

#### Tarefa 0.6: Fix #2.4 - Usar Relat√≥rios EDITED
**Arquivo**: `src/agent/cli/interactive_cli.py`  
**Se√ß√£o**: Carregamento de protocolo

**Adicionar fun√ß√£o**:
```python
def load_protocol_smart(protocol_path: Path) -> dict:
    """Carrega vers√£o EDITED se existir, caso contr√°rio ORIGINAL."""
    
    # Tentar vers√£o EDITED primeiro
    edited_path = protocol_path.parent / f"{protocol_path.stem}_EDITED{protocol_path.suffix}"
    
    if edited_path.exists():
        logger.info(f"‚úÖ Usando vers√£o EDITED: {edited_path.name}")
        return load_json(edited_path)
    
    logger.info(f"‚ÑπÔ∏è Vers√£o EDITED n√£o encontrada, usando ORIGINAL")
    return load_json(protocol_path)
```

**Usar no lugar de `load_json` direto**

---

#### Tarefa 0.7: Fix #3 - Simplificar Feedback UX
**Arquivo**: `src/agent/feedback/feedback_collector.py`  
**Linhas**: 150-250

**Mudan√ßa**:
```python
# ANTES:
print("""
S - Sim (Relevante)
N - N√£o (Irrelevante)
E - Editar sugest√£o
C - Adicionar coment√°rio
P - Pular (marcar como relevante)
Q - Sair do feedback (retornar ao pipeline)
""")

# DEPOIS:
print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  S - Relevante                      ‚îÇ
‚îÇ  N - Irrelevante (com coment√°rio)   ‚îÇ
‚îÇ  Q - Sair do feedback               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")

# Se N selecionado:
if choice == "N":
    comment = input("üí¨ Por que irrelevante? (opcional): ").strip()
    # Salvar feedback com comment
```

---

### FASE 1: Verificar Implementa√ß√£o Wave 2 (4-6 horas)

Se os arquivos N√ÉO existirem, criar conforme especifica√ß√£o abaixo.

#### Tarefa 1.1: Data Models
**Arquivo**: `src/agent/learning/models.py` (CRIAR)

```python
from pydantic import BaseModel, Field
from typing import Literal, Optional, List
from datetime import datetime

class HardRule(BaseModel):
    """Regra que BLOQUEIA sugest√£o automaticamente."""
    rule_id: str = Field(..., description="ID √∫nico da regra")
    rule_type: Literal[
        "reference_whitelist",
        "structural_constraint",
        "forbidden_pattern",
        "clinical_safety"
    ]
    condition: dict  # JSON-serializable validation logic
    block_message: str
    created_at: datetime = Field(default_factory=datetime.now)
    source: Literal["user_feedback", "playbook_rule", "system_constraint"]
    active: bool = True
    
class SoftRule(BaseModel):
    """Regra que FILTRA sugest√µes p√≥s-gera√ß√£o."""
    rule_id: str
    pattern: str
    confidence_threshold: float = 0.8
    filter_reason: str
    created_at: datetime = Field(default_factory=datetime.now)
    source: str
    frequency: int = 0
    
class ValidationResult(BaseModel):
    """Resultado de valida√ß√£o de sugest√£o."""
    blocked: bool
    violated_rules: List[str] = []
    reason: Optional[str] = None
    suggestion_id: Optional[str] = None
```

---

#### Tarefa 1.2: Rules Engine
**Arquivo**: `src/agent/learning/rules_engine.py` (CRIAR ou CORRIGIR)

**Funcionalidade esperada**:
- Carregar regras de arquivo JSON (`memory/hard_rules.json`)
- Validar sugest√µes contra todas as Hard Rules
- Bloquear sugest√µes que violam regras
- Adicionar/remover regras dinamicamente
- Salvar regras atualizadas

**Ver especifica√ß√£o completa no relat√≥rio de planejamento anterior**

---

#### Tarefa 1.3: Reference Validator
**Arquivo**: `src/agent/validators/reference_validator.py` (CRIAR ou CORRIGIR)

**Funcionalidade esperada**:
- Indexar se√ß√µes do playbook (headers markdown)
- Validar que refer√™ncias existem no playbook
- Fuzzy matching para sugest√µes (threshold 85%)
- Blacklist de refer√™ncias gen√©ricas ("N/A", "geral", "diversos")

---

#### Tarefa 1.4: Feedback Learner
**Arquivo**: `src/agent/learning/feedback_learner.py` (CRIAR ou CORRIGIR)

**Funcionalidade esperada**:
- Processar feedback S/N/Q
- Extrair padr√µes generaliz√°veis de feedback N
- Converter padr√µes em Hard Rules
- Adicionar regras ao Rules Engine
- Usar LLM para generaliza√ß√£o

---

#### Tarefa 1.5: Change Verifier
**Arquivo**: `src/agent/applicator/change_verifier.py` (CRIAR ou CORRIGIR)

**Funcionalidade esperada**:
- Validar protocolo reconstru√≠do contra schema Pydantic
- Validar condicionais via AST
- Verificar cross-references (UIDs v√°lidos)
- Verificar que Hard Rules foram respeitadas
- Verificar que mudan√ßas foram aplicadas

---

### FASE 2: Integra√ß√£o e Testes (2-3 horas)

#### Tarefa 2.1: Integrar Rules Engine em Enhanced Analyzer
**Arquivo**: `src/agent/analysis/enhanced.py`

```python
from ..learning.rules_engine import RulesEngine

class EnhancedAnalyzer:
    def __init__(self, ...):
        self.rules_engine = RulesEngine("memory/hard_rules.json")
    
    def _apply_hard_rules(self, suggestions: List[dict]) -> tuple[List[dict], List[dict]]:
        """Aplica hard rules ANTES de retornar sugest√µes."""
        valid = []
        blocked = []
        
        for sug in suggestions:
            result = self.rules_engine.validate_suggestion(sug)
            if result.blocked:
                blocked.append({**sug, "block_reason": result.reason})
            else:
                valid.append(sug)
        
        logger.info(f"‚úÖ Hard Rules: {len(valid)} v√°lidas, {len(blocked)} bloqueadas")
        return valid, blocked
```

---

#### Tarefa 2.2: Integrar Reference Validator
**Arquivo**: `src/agent/analysis/enhanced.py`

```python
from ..validators.reference_validator import ReferenceValidator, PlaybookIndex

class EnhancedAnalyzer:
    def __init__(self, ...):
        if self.playbook_path:
            playbook_index = PlaybookIndex(self.playbook_path)
            self.ref_validator = ReferenceValidator(playbook_index)
```

---

#### Tarefa 2.3: Integrar Feedback Learner
**Arquivo**: `src/agent/feedback/feedback_collector.py`

```python
from ..learning.feedback_learner import FeedbackLearner

class FeedbackCollector:
    def __init__(self, ...):
        self.feedback_learner = FeedbackLearner(llm_client, rules_engine)
    
    def collect_feedback(self, suggestions: List[dict]) -> dict:
        for sug in suggestions:
            feedback = self._get_user_choice()
            
            if feedback == "N":
                comment = input("üí¨ Por que irrelevante? (opcional): ").strip()
                
                # Aprender com feedback
                new_rules = self.feedback_learner.process_feedback(sug, "N", comment)
                
                for rule in new_rules:
                    self.rules_engine.add_rule(rule)
                    logger.info(f"üìö Nova regra aprendida: {rule.rule_id}")
```

---

#### Tarefa 2.4: Integrar Change Verifier no Reconstructor
**Arquivo**: `src/agent/applicator/protocol_reconstructor.py`

```python
from .change_verifier import ChangeVerifier

class ProtocolReconstructor:
    def reconstruct(self, ...):
        # ... reconstru√ß√£o ...
        
        # VALIDA√á√ÉO FINAL
        verifier = ChangeVerifier()
        report = verifier.verify_reconstruction(
            original_protocol=self.original_protocol,
            reconstructed_protocol=assembled_protocol,
            applied_suggestions=approved_suggestions,
            hard_rules=self.rules_engine.hard_rules
        )
        
        if not report.valid:
            logger.error("‚ùå PROTOCOLO INV√ÅLIDO - N√£o ser√° salvo!")
            logger.error(f"Viola√ß√µes: {report.violations}")
            return None
        
        logger.info(f"‚úÖ VALIDA√á√ÉO PASSOU")
        return assembled_protocol
```

---

### FASE 3: Testes End-to-End (1-2 horas)

#### Teste 1: Reconstruction Display
```bash
# Executar an√°lise completa com reconstru√ß√£o
python run_agent.py

# Verificar "MUDAN√áAS APLICADAS" mostra valores reais
# ‚úÖ SUCESSO: Location e description exibidos
# ‚ùå FALHA: Ainda mostra "N/A"
```

---

#### Teste 2: Learning System
```bash
# Run 1: An√°lise inicial
python run_agent.py
# Notar sugest√£o sug_010

# Fornecer feedback negativo
# Raz√£o: "autonomy invasion - priorizar X sobre Y"

# Verificar memory_qa.md
# Pattern deve ter frequency=1

# Run 2: Nova an√°lise
python run_agent.py
# sug_010 (tipo autonomy invasion) N√ÉO deve aparecer
# Logs devem mostrar "Bloqueado por padr√£o: autonomy_invasion"

# ‚úÖ SUCESSO: Sugest√£o n√£o aparece
# ‚ùå FALHA: Sugest√£o ainda aparece
```

---

#### Teste 3: Feedback UX
```bash
# Executar an√°lise com feedback
python run_agent.py

# Verificar apenas 3 op√ß√µes: S/N/Q
# ‚úÖ SUCESSO: 3 op√ß√µes
# ‚ùå FALHA: 7 op√ß√µes
```

---

## üéØ CRIT√âRIOS DE SUCESSO

### Fase 0 (Emergency Fixes)
- ‚úÖ Display mostra mudan√ßas reais (n√£o "N/A")
- ‚úÖ Threshold = 1 (padr√µes ativam imediatamente)
- ‚úÖ Filtros SEMPRE no prompt
- ‚úÖ Pattern-based filtering funciona
- ‚úÖ Relat√≥rios EDITED s√£o usados
- ‚úÖ Feedback tem 3 op√ß√µes apenas

### Fase 1 (Wave 2 Verification)
- ‚úÖ Todos os arquivos Wave 2 existem
- ‚úÖ Rules Engine funcional
- ‚úÖ Reference Validator funcional
- ‚úÖ Feedback Learner funcional
- ‚úÖ Change Verifier funcional

### Fase 2 (Integration)
- ‚úÖ Rules Engine integrado em Enhanced Analyzer
- ‚úÖ Reference Validator integrado
- ‚úÖ Feedback Learner integrado
- ‚úÖ Change Verifier integrado

### Fase 3 (End-to-End)
- ‚úÖ Sugest√µes rejeitadas N√ÉO reaparecem
- ‚úÖ Token waste reduzido 40-60%
- ‚úÖ Usu√°rio v√™ mudan√ßas aplicadas
- ‚úÖ Protocolos inv√°lidos s√£o bloqueados

---

## üö® RISCOS E MITIGA√á√ïES

### Risco 1: Arquivos Wave 2 n√£o existem
**Mitiga√ß√£o**: Criar usando especifica√ß√µes deste relat√≥rio

### Risco 2: Mudan√ßas quebram pipeline existente
**Mitiga√ß√£o**: 
- Fazer em branch separado
- Testar cada componente isoladamente
- Integra√ß√£o progressiva

### Risco 3: Performance degradada
**Mitiga√ß√£o**:
- Cachear rules engine
- Lazy loading de validators
- Benchmark antes/depois

---

## üìö REFER√äNCIAS

**Documentos de Contexto**:
- `proud-seeking-newt.md` - Documenta√ß√£o detalhada dos bugs
- `dev_history.md` - Hist√≥rico de implementa√ß√£o
- `roadmap.md` - Status das Waves
- `memory_qa.md` - Sistema de mem√≥ria atual

**Arquivos Chave**:
- `src/agent/analysis/enhanced.py` - Analyzer principal
- `src/agent/applicator/protocol_reconstructor.py` - Reconstrutor
- `src/agent/feedback/feedback_collector.py` - Coleta de feedback
- `src/agent/cli/interactive_cli.py` - CLI

---

## üìû PR√ìXIMOS PASSOS

1. **Claude Code**: Iniciar com Fase 0 (Emergency Fixes)
2. **Prioridade**: Fix #2 (Learning System) √© o mais cr√≠tico
3. **Valida√ß√£o**: Testar cada fix antes de prosseguir
4. **Comunica√ß√£o**: Reportar progresso a cada tarefa completada

---

**FIM DO RELAT√ìRIO**
