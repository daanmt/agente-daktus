# ğŸ—ï¸ PROPOSTA: Arquitetura de Dados Moderna

**Data**: 2025-12-11  
**Autor**: Arquiteto de SoluÃ§Ãµes + Product Manager  
**Contexto**: Sistema atual usa arquivos texto (memory_qa.md, TXTs), nÃ£o escalÃ¡vel  
**Objetivo**: Propor arquitetura de dados eficiente, escalÃ¡vel e consultÃ¡vel  

---

## ğŸ“Š PROBLEMA ATUAL

### Sistema de Arquivos Atual

```
AgenteV2/
â”œâ”€â”€ memory_qa.md                    # âŒ Texto livre, nÃ£o consultÃ¡vel
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ protocol_v1.0.0_20251211.txt    # âŒ Sem estrutura, nÃ£o agregÃ¡vel
â”‚   â”œâ”€â”€ protocol_v1.0.0_EDITED.json     # âœ… Estruturado mas sem metadata
â”‚   â””â”€â”€ protocol_v1.0.0_AUDIT.txt       # âŒ Texto livre
â””â”€â”€ logs/                           # âŒ NÃ£o correlacionado com anÃ¡lises
```

### Problemas Identificados

| Problema | Impacto | Severidade |
|----------|---------|------------|
| **Dados nÃ£o-consultÃ¡veis** | ImpossÃ­vel responder "quantas sugestÃµes de seguranÃ§a foram geradas no Ãºltimo mÃªs?" | ğŸ”´ ALTO |
| **Sem agregaÃ§Ãµes** | ImpossÃ­vel calcular ROI, trending, padrÃµes | ğŸ”´ ALTO |
| **Sem histÃ³rico temporal** | NÃ£o sabemos se qualidade estÃ¡ melhorando | ğŸ”´ ALTO |
| **Sem correlaÃ§Ã£o** | NÃ£o conseguimos ligar feedback â†’ regra â†’ bloqueio | ğŸ”´ ALTO |
| **Memory_qa.md frÃ¡gil** | Parsing complexo, propenso a erro | âš ï¸ MÃ‰DIO |
| **Sem transaÃ§Ãµes** | InconsistÃªncias em caso de crash | âš ï¸ MÃ‰DIO |
| **Sem backup estruturado** | Perda de aprendizado se arquivo corrompe | âš ï¸ MÃ‰DIO |

---

## ğŸ¯ OBJETIVOS DA NOVA ARQUITETURA

### Requisitos Funcionais

1. **Consultabilidade**: Queries SQL para responder perguntas de negÃ³cio
2. **AgregaÃ§Ãµes**: MÃ©tricas de tendÃªncia, ROI, qualidade
3. **Rastreabilidade**: Ligar feedback â†’ regra â†’ bloqueio â†’ sugestÃ£o
4. **Escalabilidade**: Suportar 1000+ anÃ¡lises sem degradaÃ§Ã£o
5. **Integridade**: TransaÃ§Ãµes ACID, sem inconsistÃªncias
6. **Backup**: Sistema de backup automatizado

### Requisitos NÃ£o-Funcionais

1. **Performance**: Queries <100ms para dashboards
2. **Compatibilidade**: Manter backward compatibility com sistema atual
3. **MigraÃ§Ã£o**: Migrar dados histÃ³ricos sem perda
4. **Simplicidade**: SQLite (sem servidor), fÃ¡cil de gerenciar

---

## ğŸ—ï¸ ARQUITETURA PROPOSTA: HÃ­brida (SQLite + Arquivos)

### PrincÃ­pio de Design

**"Structured Data in DB, Artifacts in Files"**

- **SQLite**: Dados estruturados (anÃ¡lises, feedbacks, regras, mÃ©tricas)
- **Arquivos**: Artifacts grandes (JSONs reconstruÃ­dos, relatÃ³rios audit)
- **VÃ­nculo**: Foreign keys ligam DB records a file paths

### Diagrama de Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENTE DAKTUS QA                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA LAYER (Hybrid)                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   SQLite Database      â”‚    â”‚    File System         â”‚  â”‚
â”‚  â”‚   (daktus.db)          â”‚    â”‚    (artifacts/)        â”‚  â”‚
â”‚  â”‚                        â”‚    â”‚                        â”‚  â”‚
â”‚  â”‚ - protocols            â”‚â”€â”€â”€â–¶â”‚ - protocol JSONs       â”‚  â”‚
â”‚  â”‚ - analyses             â”‚â”€â”€â”€â–¶â”‚ - audit reports        â”‚  â”‚
â”‚  â”‚ - suggestions          â”‚    â”‚ - edited protocols     â”‚  â”‚
â”‚  â”‚ - feedbacks            â”‚    â”‚                        â”‚  â”‚
â”‚  â”‚ - rules                â”‚    â”‚                        â”‚  â”‚
â”‚  â”‚ - metrics              â”‚    â”‚                        â”‚  â”‚
â”‚  â”‚ - sessions             â”‚    â”‚                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ SCHEMA DO BANCO DE DADOS

### Tabela 1: protocols

**PropÃ³sito**: Armazenar metadata de protocolos clÃ­nicos

```sql
CREATE TABLE protocols (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    protocol_name TEXT NOT NULL,              -- "amil_ficha_cardiologia"
    version TEXT NOT NULL,                    -- "1.0.0", "1.0.1"
    specialty TEXT,                           -- "cardiologia", "pre-natal"
    company TEXT,                             -- "Amil", "Athena"
    
    -- File paths
    original_file_path TEXT NOT NULL,         -- "models_json/protocol.json"
    current_file_path TEXT NOT NULL,          -- "models_json/protocol_v1.0.1.json"
    
    -- Metadata
    node_count INTEGER,
    question_count INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- Status
    status TEXT DEFAULT 'active',             -- 'active', 'archived', 'deprecated'
    
    UNIQUE(protocol_name, version)
);

CREATE INDEX idx_protocols_name ON protocols(protocol_name);
CREATE INDEX idx_protocols_status ON protocols(status);
```

**Queries Ãºteis**:
```sql
-- Protocolos mais analisados
SELECT protocol_name, COUNT(*) as analysis_count
FROM analyses
GROUP BY protocol_name
ORDER BY analysis_count DESC;

-- EvoluÃ§Ã£o de versÃµes
SELECT protocol_name, version, created_at
FROM protocols
WHERE protocol_name = 'amil_ficha_cardiologia'
ORDER BY created_at;
```

---

### Tabela 2: playbooks

**PropÃ³sito**: Armazenar metadata de playbooks clÃ­nicos

```sql
CREATE TABLE playbooks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    playbook_name TEXT NOT NULL,
    file_path TEXT NOT NULL,
    specialty TEXT,
    
    -- Content metadata
    section_count INTEGER,
    word_count INTEGER,
    
    -- Timestamps
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    UNIQUE(playbook_name)
);
```

---

### Tabela 3: analyses

**PropÃ³sito**: Registrar cada execuÃ§Ã£o de anÃ¡lise

```sql
CREATE TABLE analyses (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT NOT NULL,                 -- UUID da sessÃ£o
    
    -- Protocol & Playbook
    protocol_id INTEGER NOT NULL,
    playbook_id INTEGER,                      -- NULL se sem playbook
    
    -- LLM
    model_used TEXT NOT NULL,                 -- "claude-sonnet-4.5"
    
    -- Results
    total_suggestions INTEGER DEFAULT 0,
    high_priority INTEGER DEFAULT 0,
    medium_priority INTEGER DEFAULT 0,
    low_priority INTEGER DEFAULT 0,
    
    -- Cost
    input_tokens INTEGER DEFAULT 0,
    output_tokens INTEGER DEFAULT 0,
    estimated_cost REAL DEFAULT 0.0,
    actual_cost REAL DEFAULT 0.0,
    
    -- Timing
    duration_seconds REAL,
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    
    -- Status
    status TEXT DEFAULT 'pending',            -- 'pending', 'completed', 'failed', 'cancelled'
    error_message TEXT,
    
    -- Report paths
    report_txt_path TEXT,
    report_json_path TEXT,
    
    FOREIGN KEY (protocol_id) REFERENCES protocols(id),
    FOREIGN KEY (playbook_id) REFERENCES playbooks(id)
);

CREATE INDEX idx_analyses_session ON analyses(session_id);
CREATE INDEX idx_analyses_protocol ON analyses(protocol_id);
CREATE INDEX idx_analyses_status ON analyses(status);
CREATE INDEX idx_analyses_date ON analyses(started_at);
```

**Queries Ãºteis**:
```sql
-- Custo total por mÃªs
SELECT 
    strftime('%Y-%m', started_at) as month,
    SUM(actual_cost) as total_cost,
    COUNT(*) as analysis_count
FROM analyses
WHERE status = 'completed'
GROUP BY month
ORDER BY month DESC;

-- AnÃ¡lises mais caras
SELECT 
    p.protocol_name,
    a.actual_cost,
    a.total_suggestions,
    a.model_used
FROM analyses a
JOIN protocols p ON a.protocol_id = p.id
WHERE a.status = 'completed'
ORDER BY a.actual_cost DESC
LIMIT 10;

-- Taxa de sucesso
SELECT 
    status,
    COUNT(*) as count,
    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM analyses), 2) as percentage
FROM analyses
GROUP BY status;
```

---

### Tabela 4: suggestions

**PropÃ³sito**: Armazenar cada sugestÃ£o gerada

```sql
CREATE TABLE suggestions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    analysis_id INTEGER NOT NULL,
    
    -- Suggestion metadata
    suggestion_id TEXT NOT NULL,              -- "sug_001"
    title TEXT NOT NULL,
    description TEXT,
    category TEXT,                            -- "seguranca", "economia", "eficiencia"
    priority TEXT,                            -- "alta", "media", "baixa"
    
    -- Impact scores
    safety_score INTEGER,                     -- 0-10
    economy_level TEXT,                       -- "L", "M", "A"
    efficiency_score INTEGER,                 -- 0-10
    usability_score INTEGER,                  -- 0-10
    
    -- Location
    node_id TEXT,
    question_uid TEXT,
    json_path TEXT,
    
    -- Playbook reference
    playbook_reference TEXT,
    playbook_reference_valid BOOLEAN,         -- Validado pelo Reference Validator
    
    -- Implementation
    modification_type TEXT,                   -- "add_option", "modify_condition", etc.
    proposed_value TEXT,
    
    -- User feedback
    feedback TEXT,                            -- "relevant", "irrelevant", "questionable", NULL
    feedback_comment TEXT,
    feedback_timestamp TIMESTAMP,
    
    -- Reconstruction
    applied BOOLEAN DEFAULT FALSE,
    applied_at TIMESTAMP,
    
    FOREIGN KEY (analysis_id) REFERENCES analyses(id)
);

CREATE INDEX idx_suggestions_analysis ON suggestions(analysis_id);
CREATE INDEX idx_suggestions_feedback ON suggestions(feedback);
CREATE INDEX idx_suggestions_category ON suggestions(category);
CREATE INDEX idx_suggestions_applied ON suggestions(applied);
```

**Queries Ãºteis**:
```sql
-- Taxa de aceitaÃ§Ã£o por categoria
SELECT 
    category,
    COUNT(*) as total,
    SUM(CASE WHEN feedback = 'relevant' THEN 1 ELSE 0 END) as accepted,
    ROUND(SUM(CASE WHEN feedback = 'relevant' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as acceptance_rate
FROM suggestions
WHERE feedback IS NOT NULL
GROUP BY category
ORDER BY acceptance_rate DESC;

-- SugestÃµes mais rejeitadas (por padrÃ£o)
SELECT 
    title,
    COUNT(*) as rejection_count,
    GROUP_CONCAT(DISTINCT feedback_comment) as reasons
FROM suggestions
WHERE feedback = 'irrelevant'
GROUP BY title
HAVING rejection_count > 2
ORDER BY rejection_count DESC;

-- ROI por categoria (sugestÃµes aplicadas / custo)
SELECT 
    s.category,
    COUNT(*) as applied_count,
    AVG(s.safety_score) as avg_safety,
    AVG(a.actual_cost) as avg_cost_per_analysis
FROM suggestions s
JOIN analyses a ON s.analysis_id = a.id
WHERE s.applied = TRUE
GROUP BY s.category;
```

---

### Tabela 5: rules

**PropÃ³sito**: Armazenar Hard Rules e Soft Rules aprendidas

```sql
CREATE TABLE rules (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    rule_id TEXT UNIQUE NOT NULL,             -- "fb_20251211_001"
    rule_type TEXT NOT NULL,                  -- "hard" ou "soft"
    
    -- Rule details
    classification TEXT NOT NULL,             -- "reference_whitelist", "forbidden_pattern", etc.
    condition TEXT NOT NULL,                  -- JSON string
    block_message TEXT,
    
    -- Source
    source TEXT NOT NULL,                     -- "user_feedback", "playbook_rule", "system_constraint"
    source_feedback_id INTEGER,               -- FK para feedback que gerou a regra
    
    -- Status
    active BOOLEAN DEFAULT TRUE,
    activation_count INTEGER DEFAULT 0,       -- Quantas vezes foi ativada
    
    -- Timestamps
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    deactivated_at TIMESTAMP,
    
    FOREIGN KEY (source_feedback_id) REFERENCES suggestions(id)
);

CREATE INDEX idx_rules_type ON rules(rule_type);
CREATE INDEX idx_rules_active ON rules(active);
CREATE INDEX idx_rules_source ON rules(source);
```

**Queries Ãºteis**:
```sql
-- Regras mais ativadas
SELECT 
    rule_id,
    classification,
    activation_count,
    block_message
FROM rules
WHERE active = TRUE
ORDER BY activation_count DESC
LIMIT 10;

-- Regras aprendidas com feedback
SELECT 
    r.rule_id,
    r.block_message,
    s.feedback_comment as original_feedback,
    r.activation_count,
    r.created_at
FROM rules r
LEFT JOIN suggestions s ON r.source_feedback_id = s.id
WHERE r.source = 'user_feedback'
ORDER BY r.created_at DESC;

-- EficÃ¡cia de regras (bloqueios / tentativas)
SELECT 
    rule_id,
    block_message,
    activation_count
FROM rules
WHERE activation_count > 0
ORDER BY activation_count DESC;
```

---

### Tabela 6: reconstructions

**PropÃ³sito**: Registrar cada reconstruÃ§Ã£o de protocolo

```sql
CREATE TABLE reconstructions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    analysis_id INTEGER NOT NULL,
    
    -- Protocol versions
    source_protocol_id INTEGER NOT NULL,
    source_version TEXT NOT NULL,
    target_version TEXT NOT NULL,              -- "1.0.1", "2.0.0"
    version_increment TEXT NOT NULL,           -- "PATCH", "MINOR", "MAJOR"
    
    -- Suggestions applied
    total_suggestions INTEGER DEFAULT 0,
    applied_suggestions INTEGER DEFAULT 0,
    failed_suggestions INTEGER DEFAULT 0,
    
    -- Files
    reconstructed_file_path TEXT NOT NULL,
    audit_report_path TEXT,
    
    -- Validation
    validation_status TEXT DEFAULT 'pending',  -- 'pending', 'passed', 'failed'
    validation_errors TEXT,                    -- JSON array
    
    -- Timing
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    duration_seconds REAL,
    
    -- Status
    status TEXT DEFAULT 'pending',             -- 'pending', 'completed', 'failed'
    error_message TEXT,
    
    FOREIGN KEY (analysis_id) REFERENCES analyses(id),
    FOREIGN KEY (source_protocol_id) REFERENCES protocols(id)
);

CREATE INDEX idx_reconstructions_analysis ON reconstructions(analysis_id);
CREATE INDEX idx_reconstructions_status ON reconstructions(status);
```

**Queries Ãºteis**:
```sql
-- Taxa de sucesso de reconstruÃ§Ã£o
SELECT 
    validation_status,
    COUNT(*) as count,
    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM reconstructions), 2) as percentage
FROM reconstructions
GROUP BY validation_status;

-- Protocolos com mais reconstruÃ§Ãµes
SELECT 
    p.protocol_name,
    COUNT(r.id) as reconstruction_count,
    AVG(r.applied_suggestions) as avg_applied
FROM reconstructions r
JOIN protocols p ON r.source_protocol_id = p.id
WHERE r.status = 'completed'
GROUP BY p.protocol_name
ORDER BY reconstruction_count DESC;

-- EvoluÃ§Ã£o de versÃµes
SELECT 
    p.protocol_name,
    r.source_version,
    r.target_version,
    r.version_increment,
    r.applied_suggestions,
    r.completed_at
FROM reconstructions r
JOIN protocols p ON r.source_protocol_id = p.id
WHERE r.status = 'completed'
ORDER BY p.protocol_name, r.completed_at;
```

---

### Tabela 7: sessions

**PropÃ³sito**: Agregar informaÃ§Ãµes de sessÃ£o (anÃ¡lise + feedback + reconstruÃ§Ã£o)

```sql
CREATE TABLE sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT UNIQUE NOT NULL,          -- UUID
    
    -- User info (se aplicÃ¡vel)
    user_id TEXT,
    user_name TEXT,
    
    -- Session flow
    analysis_id INTEGER,
    reconstruction_id INTEGER,
    
    -- Timing
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ended_at TIMESTAMP,
    duration_seconds REAL,
    
    -- Status
    status TEXT DEFAULT 'active',             -- 'active', 'completed', 'abandoned'
    
    FOREIGN KEY (analysis_id) REFERENCES analyses(id),
    FOREIGN KEY (reconstruction_id) REFERENCES reconstructions(id)
);

CREATE INDEX idx_sessions_user ON sessions(user_id);
CREATE INDEX idx_sessions_status ON sessions(status);
CREATE INDEX idx_sessions_date ON sessions(started_at);
```

---

### Tabela 8: metrics

**PropÃ³sito**: Armazenar mÃ©tricas agregadas para dashboards

```sql
CREATE TABLE metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    metric_type TEXT NOT NULL,                -- "daily_cost", "acceptance_rate", "avg_suggestions"
    metric_date DATE NOT NULL,
    metric_value REAL NOT NULL,
    metric_metadata TEXT,                     -- JSON com detalhes extras
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    UNIQUE(metric_type, metric_date)
);

CREATE INDEX idx_metrics_type_date ON metrics(metric_type, metric_date);
```

**Uso**:
```sql
-- Inserir mÃ©trica diÃ¡ria
INSERT INTO metrics (metric_type, metric_date, metric_value, metric_metadata)
VALUES (
    'daily_cost',
    DATE('now'),
    (SELECT SUM(actual_cost) FROM analyses WHERE DATE(started_at) = DATE('now')),
    json_object('analysis_count', (SELECT COUNT(*) FROM analyses WHERE DATE(started_at) = DATE('now')))
);

-- Trending de custo
SELECT 
    metric_date,
    metric_value as daily_cost
FROM metrics
WHERE metric_type = 'daily_cost'
ORDER BY metric_date DESC
LIMIT 30;
```

---

## ğŸ“¦ CAMADA DE ABSTRAÃ‡ÃƒO: Data Access Layer (DAL)

### Arquitetura do DAL

```python
# src/agent/data/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ database.py          # ConexÃ£o e inicializaÃ§Ã£o
â”œâ”€â”€ models.py            # SQLAlchemy models (opcional)
â”œâ”€â”€ repositories/
â”‚   â”œâ”€â”€ protocol_repo.py
â”‚   â”œâ”€â”€ analysis_repo.py
â”‚   â”œâ”€â”€ suggestion_repo.py
â”‚   â”œâ”€â”€ rule_repo.py
â”‚   â””â”€â”€ metrics_repo.py
â””â”€â”€ migrations/
    â””â”€â”€ 001_initial_schema.sql
```

### Exemplo: AnalysisRepository

```python
# src/agent/data/repositories/analysis_repo.py

from typing import Optional, List
from datetime import datetime
import sqlite3
from pathlib import Path

class AnalysisRepository:
    """Repository para operaÃ§Ãµes de anÃ¡lise."""
    
    def __init__(self, db_path: str = "daktus.db"):
        self.db_path = db_path
    
    def create(
        self,
        session_id: str,
        protocol_id: int,
        playbook_id: Optional[int],
        model_used: str
    ) -> int:
        """Cria nova anÃ¡lise e retorna ID."""
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO analyses (
                    session_id, protocol_id, playbook_id, model_used, started_at, status
                ) VALUES (?, ?, ?, ?, ?, ?)
            """, (session_id, protocol_id, playbook_id, model_used, datetime.now(), 'pending'))
            
            return cursor.lastrowid
    
    def update_completion(
        self,
        analysis_id: int,
        total_suggestions: int,
        high_priority: int,
        medium_priority: int,
        low_priority: int,
        input_tokens: int,
        output_tokens: int,
        actual_cost: float,
        duration_seconds: float,
        report_txt_path: str,
        report_json_path: str
    ):
        """Atualiza anÃ¡lise ao completar."""
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                UPDATE analyses SET
                    total_suggestions = ?,
                    high_priority = ?,
                    medium_priority = ?,
                    low_priority = ?,
                    input_tokens = ?,
                    output_tokens = ?,
                    actual_cost = ?,
                    duration_seconds = ?,
                    completed_at = ?,
                    status = 'completed',
                    report_txt_path = ?,
                    report_json_path = ?
                WHERE id = ?
            """, (
                total_suggestions, high_priority, medium_priority, low_priority,
                input_tokens, output_tokens, actual_cost, duration_seconds,
                datetime.now(), report_txt_path, report_json_path, analysis_id
            ))
    
    def get_by_id(self, analysis_id: int) -> Optional[dict]:
        """Busca anÃ¡lise por ID."""
        
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM analyses WHERE id = ?", (analysis_id,))
            row = cursor.fetchone()
            
            return dict(row) if row else None
    
    def get_recent(self, limit: int = 10) -> List[dict]:
        """Busca anÃ¡lises recentes."""
        
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute("""
                SELECT * FROM analyses
                ORDER BY started_at DESC
                LIMIT ?
            """, (limit,))
            
            return [dict(row) for row in cursor.fetchall()]
    
    def get_cost_by_month(self, year: int, month: int) -> float:
        """Calcula custo total do mÃªs."""
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT SUM(actual_cost) as total
                FROM analyses
                WHERE strftime('%Y', started_at) = ?
                  AND strftime('%m', started_at) = ?
                  AND status = 'completed'
            """, (str(year), f"{month:02d}"))
            
            result = cursor.fetchone()
            return result[0] if result[0] else 0.0
```

---

## ğŸ”„ MIGRAÃ‡ÃƒO: Sistema Atual â†’ Novo Sistema

### EstratÃ©gia de MigraÃ§Ã£o

**Fase 1: Dual-Write** (1-2 semanas)
- Sistema grava em AMBOS (arquivos + DB)
- Leitura ainda usa arquivos (backward compatibility)
- Zero breaking changes

**Fase 2: Dual-Read** (1 semana)
- Sistema tenta ler do DB primeiro
- Fallback para arquivos se nÃ£o encontrar
- ValidaÃ§Ã£o de consistÃªncia

**Fase 3: DB-Only** (1 semana)
- Sistema usa apenas DB
- Arquivos mantidos apenas para artifacts (JSONs, PDFs)
- MigraÃ§Ã£o histÃ³rica completa

### Script de MigraÃ§Ã£o: memory_qa.md â†’ Database

```python
# scripts/migrate_memory_to_db.py

import re
from datetime import datetime
from agent.data.repositories.rule_repo import RuleRepository

def parse_memory_qa(memory_path: str) -> List[dict]:
    """Parse memory_qa.md para extrair regras."""
    
    with open(memory_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    rules = []
    
    # Exemplo de padrÃ£o:
    # ## PadrÃ£o: InvasÃ£o da Autonomia MÃ©dica
    # FrequÃªncia: 3
    # DescriÃ§Ã£o: SugestÃµes que tentam impor escolhas clÃ­nicas
    
    pattern = r'## PadrÃ£o: (.+?)\nFrequÃªncia: (\d+)\nDescriÃ§Ã£o: (.+?)(?=\n##|\Z)'
    
    for match in re.finditer(pattern, content, re.DOTALL):
        pattern_name = match.group(1).strip()
        frequency = int(match.group(2))
        description = match.group(3).strip()
        
        rules.append({
            'rule_id': f"migrated_{pattern_name.lower().replace(' ', '_')}",
            'rule_type': 'soft',
            'classification': 'forbidden_pattern',
            'condition': {'pattern': pattern_name},
            'block_message': description,
            'source': 'user_feedback',
            'activation_count': frequency,
            'created_at': datetime.now()
        })
    
    return rules

def migrate():
    """Executa migraÃ§Ã£o completa."""
    
    repo = RuleRepository()
    rules = parse_memory_qa('memory_qa.md')
    
    for rule in rules:
        repo.create(**rule)
        print(f"âœ… Migrado: {rule['rule_id']}")
    
    print(f"\nğŸ‰ MigraÃ§Ã£o completa: {len(rules)} regras migradas")

if __name__ == "__main__":
    migrate()
```

---

## ğŸ“Š DASHBOARD & ANALYTICS

### Queries Ãšteis para Dashboard

#### 1. KPIs Principais
```sql
-- KPI: Custo total (mÃªs atual)
SELECT SUM(actual_cost) as total_cost
FROM analyses
WHERE strftime('%Y-%m', started_at) = strftime('%Y-%m', 'now')
  AND status = 'completed';

-- KPI: Taxa de aceitaÃ§Ã£o
SELECT 
    ROUND(
        SUM(CASE WHEN feedback = 'relevant' THEN 1 ELSE 0 END) * 100.0 / COUNT(*),
        2
    ) as acceptance_rate
FROM suggestions
WHERE feedback IS NOT NULL;

-- KPI: ROI (sugestÃµes aplicadas / custo)
SELECT 
    COUNT(*) as applied_suggestions,
    SUM(a.actual_cost) as total_cost,
    ROUND(COUNT(*) * 1.0 / SUM(a.actual_cost), 2) as roi
FROM suggestions s
JOIN analyses a ON s.analysis_id = a.id
WHERE s.applied = TRUE;

-- KPI: Tempo mÃ©dio de anÃ¡lise
SELECT 
    AVG(duration_seconds) as avg_seconds,
    ROUND(AVG(duration_seconds) / 60, 2) as avg_minutes
FROM analyses
WHERE status = 'completed';
```

#### 2. Trending (Ãºltimos 30 dias)
```sql
-- Custo diÃ¡rio
SELECT 
    DATE(started_at) as date,
    SUM(actual_cost) as daily_cost
FROM analyses
WHERE started_at >= DATE('now', '-30 days')
  AND status = 'completed'
GROUP BY DATE(started_at)
ORDER BY date;

-- SugestÃµes por categoria (Ãºltimos 30 dias)
SELECT 
    DATE(a.started_at) as date,
    s.category,
    COUNT(*) as count
FROM suggestions s
JOIN analyses a ON s.analysis_id = a.id
WHERE a.started_at >= DATE('now', '-30 days')
GROUP BY DATE(a.started_at), s.category
ORDER BY date, category;
```

#### 3. Top/Bottom Lists
```sql
-- Top 10 protocolos por custo
SELECT 
    p.protocol_name,
    COUNT(a.id) as analysis_count,
    SUM(a.actual_cost) as total_cost,
    AVG(a.actual_cost) as avg_cost
FROM analyses a
JOIN protocols p ON a.protocol_id = p.id
WHERE a.status = 'completed'
GROUP BY p.protocol_name
ORDER BY total_cost DESC
LIMIT 10;

-- Top 10 regras mais ativadas
SELECT 
    rule_id,
    block_message,
    activation_count
FROM rules
WHERE active = TRUE
ORDER BY activation_count DESC
LIMIT 10;

-- Piores categorias de sugestÃµes (menor taxa de aceitaÃ§Ã£o)
SELECT 
    category,
    COUNT(*) as total,
    SUM(CASE WHEN feedback = 'relevant' THEN 1 ELSE 0 END) as accepted,
    ROUND(
        SUM(CASE WHEN feedback = 'relevant' THEN 1 ELSE 0 END) * 100.0 / COUNT(*),
        2
    ) as acceptance_rate
FROM suggestions
WHERE feedback IS NOT NULL
GROUP BY category
ORDER BY acceptance_rate ASC
LIMIT 10;
```

---

## ğŸš€ IMPLEMENTAÃ‡ÃƒO: Roadmap

### FASE 1: Setup Inicial (1-2 dias)

**Tarefas**:
1. Criar schema inicial (`daktus.db`)
2. Criar Data Access Layer (repositories)
3. Criar scripts de migraÃ§Ã£o
4. Testes unitÃ¡rios de repositories

**EntregÃ¡veis**:
- âœ… `src/agent/data/database.py`
- âœ… `src/agent/data/repositories/`
- âœ… `scripts/migrate_memory_to_db.py`
- âœ… `tests/test_repositories.py`

---

### FASE 2: Dual-Write (3-5 dias)

**Tarefas**:
1. Integrar AnalysisRepository em Enhanced Analyzer
2. Integrar SuggestionRepository em Feedback Collector
3. Integrar RuleRepository em Rules Engine
4. Manter escrita em arquivos (backward compatibility)

**Exemplo**:
```python
# src/agent/analysis/enhanced.py

from ..data.repositories.analysis_repo import AnalysisRepository

class EnhancedAnalyzer:
    def __init__(self, ...):
        self.analysis_repo = AnalysisRepository()
    
    def analyze(self, ...):
        # Criar registro de anÃ¡lise
        analysis_id = self.analysis_repo.create(
            session_id=self.session_id,
            protocol_id=self._get_protocol_id(),
            playbook_id=self._get_playbook_id(),
            model_used=self.model
        )
        
        # ... anÃ¡lise ...
        
        # Atualizar com resultados
        self.analysis_repo.update_completion(
            analysis_id=analysis_id,
            total_suggestions=len(suggestions),
            ...
        )
        
        # TAMBÃ‰M grava arquivo TXT (dual-write)
        self._save_report_txt(...)
```

---

### FASE 3: Dual-Read (1-2 dias)

**Tarefas**:
1. Sistema tenta ler do DB primeiro
2. Fallback para arquivos se nÃ£o encontrar
3. ValidaÃ§Ã£o de consistÃªncia

```python
def load_analysis_results(analysis_id: str):
    """Carrega resultados de anÃ¡lise."""
    
    # Tentar DB primeiro
    repo = AnalysisRepository()
    result = repo.get_by_id(analysis_id)
    
    if result:
        logger.info("âœ… Loaded from database")
        return result
    
    # Fallback para arquivo TXT
    logger.warning("âš ï¸ Not found in DB, loading from TXT")
    return load_from_txt(f"reports/analysis_{analysis_id}.txt")
```

---

### FASE 4: DB-Only (1-2 dias)

**Tarefas**:
1. Remover fallbacks para arquivos
2. Migrar histÃ³rico completo
3. ValidaÃ§Ã£o final

---

### FASE 5: Dashboard (3-5 dias - OPCIONAL)

**Tarefas**:
1. Criar CLI de analytics
2. Ou criar dashboard web (Streamlit/Flask)

**Exemplo CLI**:
```bash
# Ver KPIs do mÃªs
python -m agent.analytics kpis --month 2025-12

# Ver trending de custo
python -m agent.analytics trending --days 30

# Ver top protocolos
python -m agent.analytics top-protocols --limit 10
```

---

## ğŸ’° ANÃLISE CUSTO-BENEFÃCIO

### Custos

| Item | EsforÃ§o | Custo |
|------|---------|-------|
| Setup inicial | 1-2 dias | ğŸŸ¢ BAIXO |
| Dual-write | 3-5 dias | ğŸŸ¡ MÃ‰DIO |
| MigraÃ§Ã£o | 2-3 dias | ğŸŸ¡ MÃ‰DIO |
| Dashboard | 3-5 dias | ğŸŸ¡ MÃ‰DIO (opcional) |
| **TOTAL** | **9-15 dias** | **MÃ‰DIO** |

### BenefÃ­cios

| BenefÃ­cio | Impacto | Valor |
|-----------|---------|-------|
| **Consultabilidade** | Responder perguntas de negÃ³cio em segundos | ğŸ”´ ALTO |
| **AgregaÃ§Ãµes** | KPIs, ROI, trending automÃ¡ticos | ğŸ”´ ALTO |
| **Rastreabilidade** | Audit trail completo | ğŸ”´ ALTO |
| **Escalabilidade** | Suporta 1000+ anÃ¡lises | ğŸ”´ ALTO |
| **Integridade** | Zero inconsistÃªncias | ğŸŸ¡ MÃ‰DIO |
| **Backup** | Backup estruturado | ğŸŸ¡ MÃ‰DIO |

### ROI Estimado

**CenÃ¡rio**: 100 anÃ¡lises/mÃªs

**Antes**:
- Tempo para responder "qual o custo do Ãºltimo mÃªs?" â†’ 30 min (parsing manual)
- Tempo para calcular ROI â†’ 1 hora (cruzamento manual)
- Risco de inconsistÃªncia â†’ ALTO

**Depois**:
- Tempo para responder "qual o custo do Ãºltimo mÃªs?" â†’ 2 segundos (SQL query)
- Tempo para calcular ROI â†’ 2 segundos (SQL query)
- Risco de inconsistÃªncia â†’ ZERO

**Economia**: 10-15 horas/mÃªs em anÃ¡lise manual

---

## ğŸ¯ RECOMENDAÃ‡ÃƒO FINAL

### EstratÃ©gia Sugerida

**OPÃ‡ÃƒO 1: ImplementaÃ§Ã£o Completa (RECOMENDADO)**
- Todas as 5 fases
- Timeline: 2-3 semanas
- Valor: ROI claro, sistema escalÃ¡vel

**OPÃ‡ÃƒO 2: MVP (Alternativa para validaÃ§Ã£o rÃ¡pida)**
- Apenas Fases 1-3 (Setup + Dual-write + Dual-read)
- Timeline: 1 semana
- Valor: Prova de conceito, valida arquitetura

**OPÃ‡ÃƒO 3: Gradual (Menor risco)**
- ComeÃ§ar apenas com `analyses` e `suggestions`
- Expandir para outras tabelas depois
- Timeline: 1-2 semanas iniciais

### PrÃ³ximos Passos Imediatos

1. **DecisÃ£o de Go/No-Go**: Aprovar arquitetura proposta
2. **PriorizaÃ§Ã£o**: Definir qual opÃ§Ã£o (Completa/MVP/Gradual)
3. **Setup**: Criar schema inicial do DB
4. **MigraÃ§Ã£o**: Script de migraÃ§Ã£o de memory_qa.md
5. **IntegraÃ§Ã£o**: Dual-write nos componentes principais

---

## ğŸ“ QUESTÃ•ES EM ABERTO

1. **Dashboard**: Precisa de interface web ou CLI Ã© suficiente?
2. **Backup**: Automatizar backup do SQLite (cron job)?
3. **Multi-user**: Sistema deve suportar mÃºltiplos usuÃ¡rios simultaneamente?
4. **Cloud**: Planos de hospedar DB em cloud (S3, GCS)?
5. **Analytics avanÃ§ado**: Precisa de ML/forecasting sobre os dados?

---

**FIM DA PROPOSTA**
